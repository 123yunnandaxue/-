# LoHa
## 摘要
传统的用low-rank分解DNN参数的方法恢复的数据往往本身就是low-rank的，即数据表征能力较差，信息有限。本文提出了一种基于hardmard乘积的re-parameterization方法称为FedPara，在即使构成matrix是low-rank的前提下，也能恢复得到full-rank的DNN权重，一来压缩率较高，二来并没有牺牲model performance，因为恢复的是full-rank因此恢复得到的数据表征能力较强。同时基于FedPara提出了用于FL personalization的算法pFedPara，并给出了不同场景下的实验效果。
## Low-rank参数化背景
传统的DNN中用low-rank分解来压缩模型的方法通常被应用于预训练好的模型，即在最小化信息损失的前提下减少model的参数数量。给定一个学好的参数矩阵 W​，low-rank分解算法希望找到一个最佳的 rank-r 估计。这种方法可以将参数矩阵的原来数量级O(mn)减少为 O(rm)+O(rn)即  O(r(m+n)) ，且可以用SVD找到闭合最优解。通常这种矩阵分解会被用在FC-layer和reshaped的卷积层kernel上。但是很显然卷积层的原始形状是4阶 tensor，因此诸如CP分解，Tucker分解的low-rank张量分解算法更加适合卷积层权重的分解。
存在问题：
* 这些传统的方法往往表征的是一个low-rank张量，因此实际表征的信息非常有限。
* 传统方法一般是训练好参数后去分解参数矩阵/张量，这样精度可能下降（取决于具体分解算法和分解模型）
## FedPara Method
本文的FedPara通过将两个low-rank的矩阵进行Hadmard积恢复得到一个full-rank的参数矩阵，这样在不增加参数数量级的情况下，能表征更丰富的数据信息。另外，FedPara试图直接在训练过程用low-rank的matrix/tensor而非训练完再分解，这样降低了由于分解模型的求解算法带来的误差。

## 结论
为了克服联邦学习中的通信瓶颈，我们提出了一种新的参数化方法FedPara及其个性化版本pFedPara。我们证明，FedPara 和 pFedPara 都可以显着降低通信开销，同时性能下降最小，有时性能优于原始对应物。即使使用强的低秩约束，FedPara 也没有低秩限制，并且可以通过我们提出的低秩 Hadamard 乘积参数化来实现全秩矩阵和张量。这些有利的特性使通信效率高的联邦学习成为可能，而以前的低秩参数化和其他联邦学习方法尚未实现这种状态。我们通过以下讨论来结束我们的工作。

讨论。FedPara 在训练期间多次进行乘法运算，包括 Hadamard 乘积，以构造层的权重。与具有任意初始化的低秩参数化相比，这些乘法可能更容易受到梯度爆炸、消失、死神经元或数值不稳定的影响。在我们的实验中，我们在使用He初始化时尚未观察到此类问题（He等人，2015）。研究适合我们模型的初始化可能会改善我们方法中潜在的不稳定性。

此外，我们还讨论了神经网络中每一层在秩中的表现力。作为层特征的另一种观点，权重和激活的统计分析也提供了初始化权重的方法（He et al.， 2015;Sitzmann et al.， 2020） 和对神经网络的理解 （De & Smith， 2020），这在这项工作中几乎没有探讨。从我们的参数化和激活中分析复合权重的统计属性将是一个有前途的未来方向，并可能为特定于 FedPara 的初始化或优化铺平道路。

通过广泛的实验，我们展示了我们的方法获得了卓越的性能改进，而且似乎没有额外的成本。然而，实际的回报存在于在训练期间从参数化中重新组合 W 的原始结构时的额外计算成本;因此，我们的方法比原来的参数化和低秩方法慢。然而，如表 7 所示，在实际 FL 场景中，计算时间并不是主导因素，而是通信成本占据了总训练时间的大部分。FedPara 提供了比所有比较方法更好的帕累托效率，因为我们的方法比低秩方法具有更高的准确性，并且比原始方法的训练时间少得多。 相比之下，与分布式学习制度中的通信时间相比，计算时间可能不可忽略。虽然我们的方法可以应用于分布式学习，但我们的方法的好处可能会在那里减少。在大规模分布式学习中提高FedPara的计算和通信效率需要进一步研究。这将是一个充满希望的未来方向。

