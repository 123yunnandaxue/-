# Hierarchical Attention Networks for Document Classification
## 摘要
我们提出了一种用于文档分类的层次注意力网络。我们的模型有两个显著的特点：（i）它**具有反映文档层次结构的层次结构**；（ii）它**在单词和句子层面应用了两个层次的注意力机制，使它在构建文档表示时能够不同地关注更多和更少重要的内容。**在六个大规模文本分类任务上进行的实验表明，所提出的体系结构在很大程度上优于以前的方法。注意力层的可视化表明，该模型选择了具有质量信息的单词和句子。
##介绍
文本分类是自然语言处理的基本任务之一。目标是为文本指定标签。它具有广泛的应用，包括主题标记、情感分类和垃圾邮件检测。传统的文本分类方法表示具有稀疏词汇特征的文档。最近的方法使用了深度学习，如卷积神经网络和基于长短期记忆的递归神经网络（LSTM）学习文本表示。尽管基于神经网络的文本分类方法非常有效，但**在本文中，我们检验了这样一种假设，即通过将文档结构的知识纳入模型架构中，可以获得更好的表示。我们模型的直觉是，并非文档的所有部分都与回答查询同等相关，确定相关部分涉及到对单词的交互进行建模，而不仅仅是孤立地存在。**
我们的**主要贡献是一种新的神经结构，即层次注意力网络（HAN），旨在捕捉关于文档结构的两个基本了解**。首先，**由于文档具有层次结构（单词形成句子，句子形成文档），我们同样通过首先构建句子的表示，然后将其聚合为文档表示来构建文档表示。其次，我们观察到一份文件中不同的单词和句子具有不同的信息量。此外，单词和句子的重要性高度依赖于上下文，即同一个单词或句子在不同的上下文中可能具有不同的重要性**。为了包括对这一事实的敏感性，我们的模型包括两个层次的注意力机制——一个在单词层面，一个在句子层面——让模型在构建文档的表示时或多或少地关注单个单词和句子。注意力机制有两个好处：它不仅通常能带来更好的表现，而且还能深入了解哪些单词和句子有助于分类决策，这在应用和分析中具有价值。
**与之前的工作的关键区别在于，我们的系统使用上下文来发现令牌序列何时相关，而不是简单地过滤上下文之外的（令牌序列）**。为了评估我们的模型与其他常见分类架构相比的性能，我们查看了六个数据集，并且我们的模型显著优于以前的方法。
## 分层注意力网络
层次注意力网络（HAN）的总体架构如下图所示。**它由几个部分组成：单词序列编码器、单词级注意力层、句子编码器和句子级注意力层。**我们将在以下各节中介绍不同组件的详细信息。
![Alt text](./1697629898865.png)
### 基于GRU的句子编码
GRU使用门控机制来跟踪序列的状态，而不使用单独的存储单元。有两种类型的门：重置门rt和更新门zt。他们共同控制如何将信息更新为状态。
### 分层注意力
在这项工作中，我们专注于文档级别的分类。假设一个文档有L个句子si，每个句子都包含Ti个单词。我们所提出的模型将原始⽂档投影到向量表示中，在此基础上我们构建了⼀个分类器来执⾏⽂档分类。在下⽂中，我们将介绍如何通过分层结构从词向量逐步构建⽂档级向量。<br>
**单词编码** 给定一个句子，首先我们通过编码矩阵把单词编码为向量，使用双向GRU通过从单词的两个方向总结信息去给单词注释，因此，需要把前后关系的信息合并起来。需要注意，我们直接使用单词嵌入。对于更完整的模型，我们可以使用GRU直接从字符中获取单词向量，类似于（Ling et al.，2015）。为了简单起见，我们省略了这一点。<br>
**对单词向量使用注意力机制**并不是所有的单词都能平等地表达句子的意思。因此，我们引入注意力机制来提取对句子意义重要的单词，并将这些信息性单词的表示聚合起来形成句子向量。也就是说，我们首先通过一层MLP将单词注释hit提供给uit作为hit的隐藏表示，然后我们将单词的重要性度量为uit与单词级别的相似性上下文向量uw，并通过softmax函数获得归一化的重要性权重ait。之后，我们计算句子向量si，作为基于权重的单词注释的加权和。上下文向量uw可以被视为一个固定查询“什么是信息词”的高级表示，而不是记忆网络中使用的单词。单词上下文向量uw在训练过程中被随机初始化并联合学习。<br>
**句子编码**给定一个句子向量si，使用和单词编码相似的方法得到文件向量。<br>
**对句子向量使用注意力机制**为了奖励作为正确分类文档线索的句子，我们再次使用注意力机制，引入句子级上下文向量，并使用该向量来衡量句子的重要性。<br>
### 文档分类
文档向量v是文档的高级表示，并且可以用作文档分类的特征：![Alt text](./1697630914918.png)
我们使用正确标签的负对数可能性作为训练损失：![Alt text](./1697630953736.png)
## 实验
### 数据集
我们在六个大规模文档分类数据集（Yelp2013、Yelp2014、Yelp2015、IMDB review、Yahoo Answer、Amazon review）上评估了我们的模型的有效性。这些数据集可以分为两类文档分类任务：情感估计和主题分类。除非另有说明，我们将80%的数据用于训练，10%用于验证，其余10%用于测试。
### 基石
我们将HAN与几种基石方法进行了比较，包括使用神经网络、LSTM、基于单词的CNN、基于字符的CNN以及Conv GRNN、LSTM-GRNN的线性方法、SVM和段落嵌入等传统方法。
#### 线性模型
线性方法用构建的统计数据作为特征。基于多项式逻辑回归的线性分类器用于利用特征对文档进行分类。
**BOW和BOW+TFIDF**<br>
从训练集中选择50000个最频繁的单词并且每个单词的计数被使用特征。Bow+TFIF，顾名思义，使用计数的TFIDF作为特征。<br>
**n-grams and n-grams+TFIDF**使用最频繁的500000 n-grams（最多5-grams）。均值袋平均单词2vec嵌入（Mikolov et al.，2013）用作特征集。<br>
**Bag-of-means**使用平均word2vec编码作为特征集。<br>
#### SVMs
（Tang et al.，2015）中报道了基于SVM的方法，包括SVM+Unigrams、Bigrams、Text Features、AverageSG、SSWE。具体来说，Unigram和Bigrams分别使用bag of Unigram和bag of Bigrams作为特征。
#### 神经网络
*CNN-word：使用了类似（Kim，2014）的基于文字的CNN模型。<br>
*CNN-char：字符级别的CNN模型。<br>
*LSTM：将整个文档作为一个单独的序列，并使用所有单词的隐藏状态的平均值作为特征进行分类。<br>
*Conv GRNN和LSTM-GRNN：由（Tang et al.，2015）提出。他们还探索了层次结构：CNN或LSTM提供句子向量，然后门控递归神经网络（GRNN）将来自文档级向量表示的句子向量组合起来进行分类。<br>
### 模型构造与训练
我们将文档拆分为多个句子，并使用斯坦福大学的CoreNLP对每个句子进行标记。在构建词汇表时，我们只保留出现5次以上的单词，并用特殊的UNK标记替换出现5次的单词。我们通过在训练和验证分割上训练无监督的word2vec模型来获得单词编码，然后使用单词编码来初始化We。
模型的超参数在验证集上进行调整。在我们的实验中，我们将单词编码维度设置为200，将GRU维度设置为50。在这种情况下，前向和后向GRU的组合为单词/句子注释提供了100个维度。单词/句子上下文向量也具有随机初始化的维度100。
对于训练，我们使用64的小批量大小，并将长度相似（根据文档中的句子数量）的文档组织为一个批次。我们发现，长度调整可以使训练加速三倍。我们使用随机梯度下降来训练动量为0.9的所有模型。我们在验证集上使用网格搜索来选择最佳学习率。
### 结果与分析
所有数据集的实验结果如表2所示。我们称我们的模型为HN-{AVE，MAX，ATT}。这里，HN表示分层网络，AVE表示平均，MAX表示最大池，ATT表示我们提出的分层注意力模型。结果表明，HNATT在所有数据集中都具有最佳性能。改进与数据大小无关。对于较小的数据集我们的模型分别比以前的最佳基线方法好3.1%和4.1%。这一发现在其他较大的数据集中是一致的。我们的模型比以前最好的模型分别高出3.2%、3.4%和3.4%。无论任务类型如何，都会出现改进。
**从下表中我们可以看出，不探索分层文档结构的基于神经网络的方法，如LSTM、CNN-word、CNN-char，在大规模（就文档大小而言）文本分类方面与传统方法相比几乎没有优势**。
![Alt text](./1697632008197.png)
这清楚地**证明了所提出的用于HAN的全局单词和句子重要性向量的有效性**。
### 上下文相关注意力权重
如果单词本身就很重要或不重要，那么没有注意力机制的模型可能会很好地工作，因为模型可以自动为不相关的单词分配低权重，反之亦然。为了验证我们的模型可以捕捉上下文相关的单词重要性，我们根据Yelp 2013数据集的测试划分绘制了单词好和坏的注意力权重的分布图，如图3（a）和图4（a）所示。**我们可以看到，该分布为从0到1的单词分配了注意力权重。这表明我们的模型捕捉了不同的上下文，并为单词分配了上下文相关的权重。**
为了进一步说明，我们绘制了以评论评级为条件的分布图。图3和图4中的子图3（b）-（f）分别对应评级1-5。特别是，图3（b）显示，在评级为1的评论中，良好的权重集中在低端。随着评级的增加，权重分布也会增加。这意味着“好”一词在评分更高的评论中扮演着更重要的角色。我们可以在图4中观察到单词bad的相反趋势。**这证实了我们的模型可以捕捉上下文相关的单词重要性。**
![Alt text](./1697632292247.png)
![Alt text](./1697632307101.png)
### 注意力的可视化
为了验证我们的模型能够在文档中选择信息丰富的句子和单词，我们在图5和图6中对Yelp 2013和Yahoo Answers数据集中的几个文档的层次注意力层进行了可视化。
每一行都是一个句子（有时由于句子的长度，句子会溢出几行）。红色表示句子的重量，蓝色表示单词的重量。由于层次结构，我们通过句子权重来规范单词权重，以确保只强调重要句子中的重要单词。下图显示，我们的模型可以选择带有强烈情感的单词，如delicious,、amazing,、terrible及其对应的句子。含有许多单词的句子，如鸡尾酒、意大利面、主菜，都会被忽略。**注意，我们的模型不仅可以选择带有强烈情感的单词，还可以处理复杂的跨句子上下文。**
![Alt text](./1697632589099.png)
## 相关工作
Kim（2014）使用神经网络进行文本分类。该体系结构是计算机视觉中使用的细胞神经网络的直接应用（LeCun et al.，1998），尽管有NLP解释。Johnson和Zhang（2014）探讨了直接使用高维一维热矢量作为输入的情况。他们发现它的性能很好。与单词级模式不同，张等人（2015）将字符级CNN应用于文本分类，并取得了有竞争力的结果。Socher等人（2013）使用递归神经网络进行文本分类。Tai等人（2015）探索句子的结构，并使用树状LSTM进行分类。也有一些工作将LSTM和CNN结构结合起来进行句子分类（Lai et al.，2015；周等人，2015）。唐等人（2015）在情感分类中使用了层次结构。他们首先使用CNN或LSTM来获得句子向量，然后使用双向门控递归神经网络来组合句子向量以获得文档向量。还有一些其他工作在序列生成（Li et al.，2015）和语言建模（Lin et al.，2014）中使用了层次结构。注意机制是由（Bahdanau et al.，2014）在机器翻译中提出的。使用编码器-解码器框架，并使用注意机制在翻译前为外语单词选择原始语言中的参考单词。徐等人（2015）在生成字幕中的单词时，使用图像字幕生成中的注意力机制来选择相关的图像区域。注意力机制的进一步用途包括解析（Vinyals等人，2014）、自然语言问答（Sukhbatar等人，2015；Kumar等人，2015年；Hermann等人，2015）和图像问答（Yang et al.，2015）。与这些作品不同的是，我们探索了一种分层的注意力机制（据我们所知，这是第一个这样的例子）。
## 总结
在本文中，我们**提出了用于文档分类的层次注意力网络（HAN）**。使用文档中信息量大的组件获得了更好的可视化效果。**我们的模型通过将重要单词聚合为句子向量，然后将重要句子向量聚合为文档向量，逐步构建文档向量**。实验结果表明，我们的模型性能明显优于以前的方法。这些注意力层的可视化表明，我们的模型在挑选重要单词和句子方面是有效的。
