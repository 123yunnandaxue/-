# LORA：大型语言模型的低秩适配
## 摘要
自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，完全微调（重新训练所有模型参数）变得不那么可行。以 GPT-3 175B 为例，部署微调模型的独立实例，每个实例都有 175B 参数，成本高得令人望而却步。我们提出了低秩自适应（Low-Rank Adaptation，简称LoRA），**它冻结了预训练模型的权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层中，大大减少了下游任务的可训练参数数量**。与使用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数的数量减少 10,000 倍，GPU 内存需求减少 3 倍。LoRA 在 RoBERTa、DeBERTa、GPT-2 和 GPT-3 上的模型质量与微调相当或更好，尽管可训练参数更少、训练吞吐量更高，并且与适配器不同，没有额外的推理延迟。我们还对语言模型适应中的秩缺陷进行了实证研究，从而阐明了 LoRA 的功效。我们发布了一个软件包，有助于将 LoRA 与 PyTorch 模型集成，并在 https://github.com/microsoft/LoRA 上为 RoBERTa、DeBERTa 和 GPT-2 提供我们的实现和模型检查点。
## 介绍
自然语言处理中的许多应用都依赖于将一个大规模的、预训练好的语言模型适应于多个下游的应用。这种适应通常是通过微调完成的，微调的缺点是新模型包含的参数和原始模型一样多。随着更大的模型每隔几个月就会被训练一次，这对于GPT-2（Radford等人，b）或RoBERTa large（Liu等人，2019）来说，仅仅是一个 "不便之处"，而对于GPT-3（Brown等人，2020）则是一个关键的部署挑战，有1750亿可训练参数。
许多人试图通过只调整一些参数或为新任务学习外部模块来缓解这一问题。这样，除了每个任务的预训练模型外，我们只需要存储和加载少量的特定任务参数，大大提升了部署时的运行效率。然而，现有的技术往往通过扩展模型深度或减少模型的可用序列长度（Li & Liang，2021；Lester等，2021；Hambardzumyan等，2020；Liu等，2021）引入推理延迟（Houlsby等，2019；Rebuffi等，2017）（第3节）。更重要的是，这些方法往往不能与微调基线相匹配，造成了效率和模型质量之间的权衡。
我们从Li等人（2018a）；Aghajanyan等人（2020）那里得到启发，他们表明学到的过度参数化模型实际上位于一个低的本征维度上。我们假设模型适应过程中权重的变化也具有较低的 "内在秩"，从而导致我们提出的低秩适应（LoRA）方法。LoRA允许我们通过优化密集层在适应过程中的变化的秩分解矩阵来间接地训练神经网络中的一些密集层，而保持预训练的权重冻结，如图1所示。以GPT-3 175B为例，我们表明，即使满秩（即d）高达12288，一个非常低的秩（即图1中的r可以是1或2）也足够了，这使得LoRA既有存储又有计算效率。<br>
![](https://pic3.zhimg.com/80/v2-39b8955f9c29803ec8f75c9d9b836e46_720w.webp)<br>
LoRA 具有几个关键优势：
* 可以共享预训练模型，并用于为不同的任务构建许多小型 LoRA 模块。通过替换图 1 中的矩阵 A 和 B，我们可以冻结共享模型并高效切换任务，从而显著降低存储需求和任务切换开销。
* LoRA 使训练更加高效，并在使用自适应优化器时将硬件入门门槛降低多达 3 倍，因为我们不需要计算梯度或维护大多数参数的优化器状态.取而代之的是，我们只优化注入的、小得多的低秩矩阵。
* 我们简单的线性设计允许我们在部署时将可训练矩阵与冻结权重合并，与完全微调的模型相比，通过构造不会引入推理延迟。
* LoRA 与许多先前的方法正交，可以与其中许多方法结合使用，例如前缀调整。我们在附录 E 中提供了一个示例。

术语和惯例 我们经常提到Transformer架构，并对其维度使用常规术语。我们把Transformer层的输入和输出维度大小称为model。我们用Wq、Wk、Wv和Wn来指代self-attention模块中的查询/键/值/输出投影矩阵。W或W0指的是预训练的权重矩阵，∆W指的是适应过程中的累积梯度更新。我们用r来表示一个LoRA模块的秩。我们遵循（Vaswani等人，2017；Brown等人，2020）规定的惯例，使用Adam（Loshchilov & Hutter，2019；Kingma & Ba，2017）进行模型优化，并使用Transformer MLP前馈维度dfn=4×dmodel。
## 问题陈述
虽然我们的建议与训练目标无关，但我们专注于语言模型作为我们的激励用例。下面是对语言模型问题的简要描述，特别是对给定特定任务提示的条件概率的最大化。假设我们得到了一个预训练的自回归语言模型 P(y|x)，并以Φ为参数。例如， P(y|x)可以是一个通用的多任务学习器，如GPT（Radford等人，b；Brown等人，2020），基于Transformer架构（Vaswani等人，2017）。考虑将这个预训练的模型调整为下游的条件文本生成任务，如总结、机器阅读理解（MRC）和自然语言转SQL（NL2SQL）。每个下游任务都由上下文-目标对的训练数据集表示。 Z={(xi，yi)}，其中xi和yi都是token的序列。例如，在NL2SQL中，xi是一个自然语言查询， yi是其相应的SQL命令；对于总结，xi是一篇文章的内容，yi是其摘要。在全面微调期间，模型被初始化为预训练的权重Φ0，并通过反复遵循梯度来更新为 Φ0+△Φ
 ，以最大化条件语言模型目标。
![](https://pic1.zhimg.com/80/v2-94800bba2372ff72a42f71b819c727f0_720w.webp)
完全微调的主要缺点之一是，对于每个下游任务，我们都要学习一组不同的参数∆Φ，其维度|∆Φ|等于|Φ0| 。因此，如果预训练的模型很大（如GPT-3的|Φ0|≈1750亿），存储和部署许多独立的微调模型实例会很有挑战性，即使是可行的。

在本文中，我们采用了一种更有效的参数方法，其中特定任务的参数增量 ∆Φ=∆Φ(Θ)被一个尺寸小得多的参数集Θ进一步编码，而|Θ|<<|Φ0| 的任务是寻找∆Φ，因此变成了对Θ进行优化。<br>
![](https://pic2.zhimg.com/80/v2-1046e10fc39b0809e5788b2410253159_720w.webp)

在接下来的章节中，我们建议使用低秩表示来编码∆Φ，这样既能提高计算效率又能提高记忆效率。当预训练模型为GPT-3 175B时，可训练参数|Θ|的数量可以小到|Φ|的0.01%。
## 现有的解决方案还不够好吗？
我们要解决的问题绝不是新问题:自从迁移学习开始以来，有几十项工作试图使模型适应性更强的参数和计算效率。参见第6节对一些著名工作的调查。以语言模型为例，当涉及到高效的适应时，有两种突出的策略：增加适应层（Houlsby等人，2019；Rebuffi等人，2017；Pfeiffer等人，2021；Ru¨ckle´等人，2020）或优化输入层激活的某些形式（Li & Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。然而，这两种策略都有其局限性，特别是在大规模和延迟敏感的生产场景中。
适配器层引入了推理延迟 适配器有很多变体。我们关注Houlsby等人（2019）的原始设计，它在每个Transformer块上有两个适配器层，以及Lin等人（2020）的最新设计，它在每个块上只有一个，但有一个额外的LayerNorm（Ba等人，2016）。虽然人们可以通过修剪层或利用多任务设置来减少整体延迟（Ru¨ckle´等人，2020；Pfeiffer等人，2021），但没有直接的方法来绕过适配器层的额外计算。这似乎不是一个问题，因为适配器层被设计成只有很少的参数（有时<1%的原始模型），其瓶颈维度很小，这限制了它们可以增加的FLOPs。然而，大型神经网络依靠硬件并行性来保持低延迟，而适配器层必须按顺序处理。这在在线推理设置中是有区别的，在这种情况下，批次大小通常小到一个。在没有模型并行的一般情况下，比如在单个GPU上运行GPT-2（Radford等人，b）媒介的推理，我们看到使用适配器时延迟明显增加，即使瓶颈维度非常小。
当我们需要像Shoeybi等人（2020）；Lepikhin等人（2020）那样对模型进行分片时，这个问题会变得更糟，因为额外的深度需要更多的同步GPU运算，如AllReduce和Broadcast，除非我们多次冗余地存储适配器参数。
直接优化提示是困难的 另一个方向，如前缀优化（Li & Liang, 2021），面临不同的挑战。我们观察到，前缀优化很难优化，其性能在可训练参数中的变化是非单调的，证实了原始论文中的类似观察。更根本的是，为适应保留一部分序列长度必然会减少可用于处理下游任务的序列长度，我们怀疑这使得调整提示与其他方法相比性能较差。
## 我们的方法
我们描述了LoRA的简单设计和它的实际好处。这里概述的原则适用于深度学习模型中的任何密集层，尽管我们在实验中只关注Transformer语言模型中的某些权重，作为激励的用例。
### 低秩参数化的更新矩阵
一个神经网络包含许多密集的层，它们进行矩阵乘法。这些层中的权重矩阵通常具有满秩。当适应一个特定的任务时，Aghajanyan等人（2020）表明，预训练语言模型具有较低的 "本征维度"，尽管随机投影到一个较小的子空间，仍然可以有效地学习。受此启发，我们假设权重的更新在适应过程中也具有较低的 "内在秩"。对于一个预训练好的权重矩阵 W0，我们通过用低秩分解 W0+△W=W0+BA代表后者来约束其更新，在训练期间，W0被冻结，不接受梯度更新，而A和B包含可训练参数。请注意，W0和 △W=BA都与相同的输入相乘，它们各自的输出向量按坐标相加。对于 h=W0x，我们修改后的前向传递产生了h=w0x+△Wx=w0x+BAx。
我们对A使用随机的高斯初始化，B在训练开始时为零。然后我们对  △W=BA 进行扩展，其中α是r中的一个常数。当用Adam进行优化时，如果我们适当地扩展初始化，调整α与调整学习率大致相同。因此，我们只需将α设置为我们尝试的第一个r，而不对其进行调整。这种比例有助于减少我们在改变r时重新调整超参数的需要（Yang & Hu, 2021）。

**完全微调的一般化**:一种更普遍的微调形式允许训练预训练参数的一个子集。LoRA更进一步，不要求权重矩阵的累积梯度更新在适应过程中具有满秩。这意味着，当对所有权重矩阵应用LoRA并训练所有偏差时，我们通过将LoRA秩r设置为预训练权重矩阵的秩，大致恢复了完全微调的表现力。换句话说，随着我们增加可训练参数的数量，训练LoRA大致收敛于训练原始模型，而基于适配器的方法收敛于MLP，基于前缀的方法收敛于不能接受长输入序列的模型。

**没有额外的推理延时**:在生产中部署时，我们可以明确地计算和存储 W=W0+BA，并像往常一样执行推理。请注意，W0和BA都在Rd×k中。当我们需要切换到另一个下游任务时，我们可以通过减去BA，然后增加一个不同的B'A'来恢复W0，这是一个快速的运算，内存开销非常小。最关键的是，这保证了我们在推理过程中，与结构上的微调模型相比，不会引入任何额外的延迟。
### 将LORA应用于transformer
原则上，我们可以将LoRA应用于神经网络中的任何权重矩阵子集，以减少可训练参数的数量。在Transformer架构中，self-attention模块中有四个权重矩阵，MLP模块中有两个。我们将Wq或Wk，Wv视为维度为 dmodel*dmodel的单一矩阵，即使输出维度通常被切成注意力头。我们将研究限制在只适应下游任务的注意力权重，并冻结MLP模块（因此它们不在下游任务中训练），这既是为了简单，也是为了参数效率。<br>
**实际的好处和限制**:最重要的好处来自于内存和存储用量的减少。对于一个用Adam训练的大型transformer，如果r << dmodel，我们减少了2/3的VRAM用量，因为我们不需要存储冻结参数的优化器状态。在GPT-3 175B上，我们将训练期间的VRAM消耗从1.2TB减少到350GB。在r=4并且只有查询和值投影矩阵被调整的情况下，checkpoint的大小大约减少了10,000×（从350GB到35MB）。 这使得我们可以用更少的GPU进行训练，避免I/O瓶颈。另一个好处是，我们可以在部署时以更低的成本切换任务，只交换LoRA的权重，而不是所有的参数。这使得我们可以创建许多定制的模型，这些模型可以在将预训练的权重存储在VRAM中的机器上进行实时切换。我们还观察到，在GPT-3 175B上训练时，与完全微调相比，速度提高了25%，因为我们不需要为绝大多数的参数计算梯度。<br>
**LoRA也有其局限性**:例如，如果选择将A和B吸收到W中以消除额外的推理延迟，那么在一次前向传递中对不同任务的输入进行批处理是不直接的。尽管在延迟不重要的情况下，可以不合并权重，动态地选择LoRA模块用于批次中的样本。
## 相关工作
Transformer语言模型，Transformer（Vaswani等人，2017）是一个seq2seq的架构，大量使用了self-attention。Radford等人（a）通过使用Transformer解码器的堆栈将其应用于自回归语言模型。从那时起，基于Transformer的语言模型主导了NLP，在许多任务中达到了最先进的水平。BERT（Devlin等人，2019b）和GPT-2（Radford等人，b）出现了一个新的范式--两者都是在大量文本上训练的大型Transformer语言模型--与直接在特定任务数据上训练相比，在一般领域数据上进行预训练后，在特定任务数据上进行微调会带来明显的性能提升。训练更大的transformer通常会带来更好的性能，这仍然是一个活跃的研究方向。GPT-3（Brown等人，2020）是迄今为止训练的最大的单一Transformer语言模型，具有175B的参数。

**提示工程和微调**:虽然GPT-3 175B可以通过一些额外的训练实例来调整其行为，但其结果在很大程度上取决于输入提示（Brown等人，2020）。这就需要一种经验性的艺术来组成和格式化提示，以最大限度地提高模型在预期任务上的表现，这被称为提示工程或提示黑客。微调将一个在一般领域中预训练的模型重新训练到一个特定的任务上 Devlin等人（2019b）；Radford等人（a）。它的变种包括只学习参数的一个子集Devlin等人（2019b）；Collobert & Weston（2008），然而，从业者经常重新训练所有的参数以最大限度地提高下游性能。然而，GPT-3 175B的艰巨性使得以通常的方式进行微调具有挑战性，因为它产生了大量的checkpoint，并且由于它具有与预训练相同的内存足迹，因此硬件门槛很高。

**参数有效的适应**:许多人提出在神经网络的现有层之间插入适配器层（Houlsby等人，2019；Rebuffi等人，2017；Lin等人，2020）。我们的方法使用类似的瓶颈结构，对权重更新施加低秩约束。关键的功能差异是，我们学习的权重可以在推理过程中与主权重合并，因此不会引入任何延迟，而适配器层则不存在这种情况（第3节）。适配器的临时扩展是COMPACTER（Mahabadi等人，2021年），它基本上是使用克朗克乘积与一些预定的权重共享方案对适配器层进行参数化。同样，将LoRA与其他基于张量积的方法相结合，有可能提高其参数效率，这一点我们留待以后的工作。最近，许多人提出优化输入词嵌入以代替微调，类似于提示工程的连续和可微调概括（Li & Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。我们在实验部分包括与Li & Liang（2021）的比较。然而，这一行的工作只能通过在提示中使用更多的特殊token来扩大规模，当位置嵌入被学习时，这些token会占用任务token的可用序列长度。

**深度学习中的低秩结构**:低秩结构在机器学习中非常常见。很多机器学习问题都有某些内在的低秩结构（Li等人，2016；Cai等人，2010；Li等人，2018b；Grasedyck等人，2013）。此外，众所周知，对于许多深度学习任务，特别是那些具有严重过度参数化的神经网络，学习的神经网络在训练后将享有低秩属性（Oymak等人，2019）。一些先前的工作甚至在训练原始神经网络时明确施加了低秩约束（Sainath等人，2013；Povey等人，2018；Zhang等人，2014；Jaderberg等人，2014；Zhao等人，2016；Khodak等人，2021；Denil等人，2014）；然而，就我们所知，这些工作中没有一项考虑对冻结模型进行低秩更新以适应下游任务。在理论文献中，众所周知，当底层概念类具有一定的低秩结构时，神经网络的表现优于其他经典的学习方法，包括相应的（有限宽度）神经切线核（Allen-Zhu等人，2019；Li & Liang，2018）（Ghorbani等人，2020；Allen-Zhu & Li，2019；Allen-Zhu & Li，2020a）。Allen-Zhu & Li（2020b）的另一个理论结果表明，低秩适应性对于对抗性训练是有用的。总而言之，我们认为我们提出的低秩适应性更新在文献中得到了很好的启发。
## 了解低秩的更新
给定LoRA的经验优势，我们希望进一步解释从下游任务中学习到的低秩适应的特性。请注意，低秩结构不仅降低了硬件门槛，使我们能够并行地运行多个实验，而且还能更好地解释更新权重与预训练权重的相关性。我们把研究重点放在GPT-3 175B上，在那里我们实现了可训练参数的最大减少（高达10,000倍），而没有对任务性能产生负面影响。

我们进行了一系列的实证研究，以回答以下问题：
* 在参数预算约束下，我们应该调整预训练的Transformer中的哪一个权重矩阵子集，以使下游性能最大化？
* "最佳 "适应矩阵∆W是否真的存在秩亏？如果是的话，在实践中使用的好秩是什么？
*  ∆W和W之间有什么联系？∆W是否与W高度相关？与W相比，∆W有多大？

我们相信，我们对问题（2）和（3）的回答阐明了在下游任务中使用预训练语言模型的基本原则，这是NLP的一个关键主题。

### 我们应该对transformer中的哪些权重矩阵应用Lora？
在有限的参数预算下，我们应该用LoRA调整哪种类型的权重，以获得下游任务的最佳性能？如第4.2节所述，我们只考虑self-attention模块的权重矩阵。我们在GPT-3 175B上设置了18M的参数预算（如果存储在FP16中大约是35MB），如果我们适应一种类型的注意力权重，相当于r=8，如果我们适应两种类型，相当于所有96层的r=4。请注意，把所有的参数放在∆Wq或∆Wk中会导致性能明显降低，而同时适应Wq和Wv会产生最好的结果。这表明，即使是四级的秩也能在∆W中捕捉到足够的信息，这样一来，适应更多的权重矩阵比适应具有较大秩的单一类型的权重更可取。
### LORA的最佳秩是什么？
我们把注意力转向秩r对模型性能的影响。我们对 Wq,Wv和{Wq,Wk,Wv,Wc}和仅仅Wq进行了调整，以进行比较。令人惊讶的是，LoRA在很小的r下已经有了很好的表现（对于{Wq, Wv}来说比Wq更有竞争力）。我们认为，增加r并不能覆盖一个更有意义的子空间，这表明低秩的适应矩阵就足够了。
## 结论和未来工作
从所需的硬件和为不同任务托管独立实例的存储/切换成本来看，微调巨大的语言模型是非常昂贵的。我们提出了LoRA，一种高效的适应策略，既不引入推理延迟，也不减少输入序列的长度，同时保留了高的模型质量。重要的是，它允许在作为服务部署时通过共享绝大多数的模型参数来实现快速的任务切换。虽然我们专注于Transformer语言模型，但提出的原则一般适用于任何具有密集层的神经网络。
未来的工作有很多方向:
1) LoRA可以与其他有效的适应方法相结合，有可能提供不相关的改进。
2) 微调或LoRA背后的机制还很不清楚--在预训练期间学到的特征是如何转化为在下游任务中表现良好的？我们相信LoRA比完全微调更容易回答这个问题。
3) 我们主要依靠启发式方法来选择要应用LoRA的权重矩阵。是否有更多原则性的方法？
4) 最后，∆W的秩亏表明，W也可能是秩亏的，这也可以成为未来工作的灵感来源。

