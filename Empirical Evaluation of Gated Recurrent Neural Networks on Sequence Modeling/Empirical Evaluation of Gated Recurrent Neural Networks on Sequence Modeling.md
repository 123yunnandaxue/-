# Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
## 摘要
在本文中，我们`比较了循环神经网络中不同类型的循环单元`。特别是，我们关注实现门控机制的更复杂的单元，例如长短期记忆（LSTM）单元和最近提出的门控递归单元（GRU）。我们在复调音乐建模和语音信号建模的任务中评估这些递归单元。我们的实验表明，`这些先进的循环单元确实比传统的循环单元（如tanh单元）要好。此外，我们发现GRU与LSTM相当。`
## 介绍
循环神经网络最近在许多机器学习任务中显示出了有希望的结果，尤其是当输入和/或输出长度可变时。最近，Sutskever等人[2014]和Bahdanau等人[2014]报告称，递归神经网络能够在机器翻译这一具有挑战性的任务中与现有的、发展良好的系统一样发挥作用。我们从最近的这些成功中得出的一个有趣的观察结果是，这些成功几乎都不是用vanilla递归神经网络实现的。相反，被用于这些成功的应用是一个具有复杂的递归隐藏单元的递归神经网络，如长短期记忆单元。
在这些复杂的递归单元中，在本文中，我们有兴趣评估两个密切相关的变体。一个是长短期记忆（LSTM）单元，另一个是门控递归，由Cho等人[2014]最近提出的单元（GRU）。LSTM单元在具有长期依赖性的基于序列的任务中工作良好，这在该领域已经得到了很好的证实，但后者直到最近才被引入并用于机器翻译的上下文中。
`在本文中，我们在序列建模任务中评估了这两个单元和一个更传统的tanh单元。`我们考虑三个复调音乐数据集以及Ubisoft提供的两个内部数据集，其中每个样本都是原始语音表示。
基于我们的实验，我们得出结论，`通过对一些数据集上的所有模型使用固定数量的参数，GRU在CPU时间的收敛性以及参数更新和泛化方面都优于LSTM单元。`
## 背景：循环神经网络RNN
循环神经网络（RNN）是传统前馈神经网络的扩展，能够处理可变长度的序列输入。RNN通过具有递归隐藏状态来处理可变长度序列，该状态在每次的激活取决于前一次的激活。RNN的结构如下图所示。和普通神经网络的区别在于，循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。
![enter image description here](https://pic2.zhimg.com/80/v2-b0175ebd3419f9a11a3d0d8b00e28675_720w.webp)
用公式表达就是：![enter image description here](https://pic4.zhimg.com/80/v2-9524a28210c98ed130644eb3c3002087_720w.webp)
## Gated Recurrent Neural Networks
在本文中，我们感兴趣的是评估最近提出的递归单元（LSTM单元和GRU）在序列建模中的性能。在进行实证评估之前，我们首先在本节中描述这些经常性单元中的每一个。
### 长短期记忆单元LSTM
以自己的语言概括的话，LSTM就是在RNN上的改进，使其拥有长期记忆的能力。传统的RNN结构可以看做是多个重复的神经元构成的“回路”，每个神经元都接受输入信息并产生输出，然后将输出再次作为下一个神经元的输入，依次传递下去。这种结构能够在序列数据上学习短时依赖关系，但是由于梯度消失和梯度爆炸问题，RNN在处理长序列时难以达到很好的性能。
而LSTM通过引入记忆细胞、输入门、输出门和遗忘门的概念，能够有效地解决长序列问题。记忆细胞负责保存重要信息，输入门决定要不要将当前输入信息写入记忆细胞，遗忘门决定要不要遗忘记忆细胞中的信息，输出门决定要不要将记忆细胞的信息作为当前的输出。这些门的控制能够有效地捕捉序列中重要的长时间依赖性，并且能够解决梯度问题。LSTM相当于在RNN的基础上增加了一个日记本，随着时间的推移对日记本中的内容进行删除和增加，因此，与传统的RNN相比，LSTM更加适用于处理和预测时间序列中间隔较长的重要事件。
### Gated Recurrent Unit 门控递归单元
GRU也属于RNN的一个变体，其结构图如下图所示。主要由更新门和重置门组成。
![enter image description here](https://pic3.zhimg.com/80/v2-2f1b182fd4b8cd86219f50f50f4816aa_1440w.webp)
其中，rt是重置门，zt是更新门。GRU的大致思路为：通过本位置输入和上层输出结合sigmoid，得到重置门和更新门。用重置门+tanh对原信息（上位置输出和本位置输入）进行变化，得到对原信息处理后的结果。将该结果经更新门选择，和上位置输出权重相加，得到本位置输出。
### 讨论
从下图很容易注意到LSTM单元和GRU之间的相似性。这些单元之间共享的最突出的功能是其更新的附加组件从t到t+1，这是传统递归单元所缺乏的。传统的递归单元总是将激活或单元的内容替换为根据当前值计算的新值输入和以前的隐藏状态。另一方面，LSTM单元和GRU都保留了现有的
内容，并在其上添加新内容。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Empirical%20Evaluation%20of%20Gated%20Recurrent%20Neural%20Networks%20on%20Sequence%20Modeling/picture/1abf2411-917a-4a72-8e2c-3f0b319be06c.png)
## 实验设置
### 任务和数据集
我们在复调音乐建模和语音信号建模任务中对这些单元进行了评估。
* 对于复调音乐建模，我们使用了三个来自[Boulanger Lewandowski et al.，2012]的复调音乐数据集：Nottingham、JSB Chorales、Muse Data和Piano midi。这些数据集包含序列，其中每个符号分别是93、96、105和108维二进制矢量。我们使用逻辑S形函数作为输出单位。
* 我们使用Ubisoft提供的两个内部数据集用于语音信号建模。每个序列都是一维原始音频信号，在每个时间步长，我们设计一个递归神经网络来观察20个连续样本，以预测接下来的10个连续样本。我们使用了两个不同版本的数据集：一个是长度为500的序列（UbisoftA），另一个是长为8000的序列（UbisoftB）。UbisoftA和UbisoftB各有7230和800个序列。我们使用具有20个成分的高斯混合作为输出层。
### 模型
对于每项任务，我们训练三个不同的递归神经网络，每个网络都有LSTM单元（LSTM-RNN）、GRU（GRU-RNN）或tanh单元。
由于这些实验的主要目的是公平地比较所有三个单元，我们选择每个模型的大小，使每个模型具有大致相同数量的参数。我们有意使模型足够小，以避免过拟合，这很容易分散比较的注意力。这种比较神经网络中不同类型隐藏单元的方法以前已经做过，例如，Gulcehre等人[2014]。结果如下表所示。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Empirical%20Evaluation%20of%20Gated%20Recurrent%20Neural%20Networks%20on%20Sequence%20Modeling/picture/bd23542a-1935-4f6e-899a-7e5bd747ec5f.png)
### 结果及分析
从上表我们可以看出，在音乐数据集的情况下，GRU-RNN在除Nottingham之外的所有数据集上都优于所有其他数据集（LSTM-RNN和tanh-RNN）。然而，我们可以看到，在这些音乐数据集上，所有三个模型的表现都非常接近。
另一方面，在Ubisoft的两个数据集上，具有门控单元的RNN（GRU-RNN和LSTM-RNN）明显优于更传统的tanh RNN。LSTM-RNN在UbisoftA中表现最好，而在UbisoftB中，GRU-RNN表现最好。
从下图的学习曲线我们可以看出，在音乐数据集的情况下，我们看到GRU-RNN在更新次数和实际CPU时间方面都取得了更快的进展。如果我们考虑Ubisoft数据集，很明显，尽管tanh RNN中每次更新的计算需求比其他模型小得多，但每次更新都没有取得多大进展，最终在更糟糕的水平上停止了任何进展。
`这些结果清楚地表明了门控单元相对于更传统的递归单元的优势:收敛往往更快，最终的解决方案往往更好。`然而，我们的结果在比较LSTM和GRU时并不是决定性的，这表明`门控递归单元类型的选择可能在很大程度上取决于数据集和相应的任务。`
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Empirical%20Evaluation%20of%20Gated%20Recurrent%20Neural%20Networks%20on%20Sequence%20Modeling/picture/54ed3747-a910-422a-8bf9-18133f34a800.png)
## 总结
在本文中，我们对具有三个广泛使用的递归单元的递归神经网络（RNN）进行了实证评估；（1） 传统的tanh单元、（2）长短期记忆（LSTM）单元和（3）最近提出的门控递归单元（GRU）。`我们的评估重点是在多个数据集上进行序列建模`，包括复调音乐数据和原始语音信号数据。`评估清楚地表明了门控单元的优越性；LSTM单元和GRU都优于传统的tanh单元。`这一点在更具挑战性的原始语音信号建模任务中更为明显。然而，`无法就两个门控单元中哪一个更好得出具体结论。`我们认为本文中的实验是初步的。为了更好地理解门控单元如何帮助学习，并分离出门控单元中每个组件的贡献，例如LSTM单元或GRU中的门控单元，未来将需要更彻底的实验。

