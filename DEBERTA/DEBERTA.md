# DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION
## 摘要
预训练神经模型的研究已经显著提高了许多自然语言处理(NLP)任务的性能。**本文提出了一种新的基于解耦注意的decoding增强BERT模型(DeBERTa)**，利用两种新技术对BERT模型和RoBERTa模型进行了改进。**第一种是解耦注意机制，其中每个单词用两个向量表示，分别对其内容和位置进行编码，单词之间的注意权重使用内容和相对位置的解耦矩阵计算。其次，在模型预训练中，利用增强掩码解码器在解码层中加入绝对位置来预测掩码token;此外，还采用了一种新的虚拟对抗训练方法进行微调，以提高模型的泛化能力**。研究表明，这些技术显著提高了模型预训练的效率，以及自然语言理解(NLU)和自然语言生成(NLG)下游任务的性能。
## 介绍
Transformer已成为用于神经语言模型的最有效的神经网络体系结构。**与按顺序处理文本的循环神经网络(RNN)不同，transformer应用self-attention功能来并行计算输入文本中的每个单词的注意力权重，从而衡量每个单词对另一个单词的影响，因此与RNN相比，它可以并行大规模模型训练**。在本文中，我们提出了一种新的基于Transformer的神经语言模型DeBERTa(具有解耦注意力的解码增强BERT)，它使用两种新颖的技术改进了以前的最新PLM：解耦注意力机制和增强的mask解码器。<br>
* **解耦注意力**: 与BERT不同，`在BERT中，输入层中的每个单词都使用一个向量来表示，该向量是其单词(内容)嵌入和位置嵌入的总和， 在DeBerta中单词间的权重分别根据其内容和相对位置使用解耦的矩阵进行计算`。 这是由于观察到一个单词对的注意力权重不仅取决于它们的内容，而且还取决于它们的相对位置。 例如，单词“deep”和“learning”相邻时的的关系要强于出现在不同句子中。
* **增强的mask解码器**: 与BERT一样，DeBERTa也使用mask语言模型(MLM)进行了预训练。 MLM是一项填空任务，在该任务中，模型被教导要使用mask token周围的单词来预测mask的单词应该是什么。 DeBERTa将上下文的内容和位置信息用于MLM。解耦注意力机制已经考虑了上下文词的内容和相对位置，但没有考虑这些词的绝对位置，这在很多情况下对于预测至关重要。考虑一下句子“a new store opened beside the new mall”，其斜体字“store”和“mall”被mask以进行预测。尽管两个单词的局部上下文相似，但是它们在句子中扮演的句法作用不同。 (这里，句子的主题是“store”而不是“mall”。)这些句法上的细微差别在很大程度上取决于单词在句子中的绝对位置，因此考虑单词在语言模型中的绝对位置是很重要的。`DeBERTa在softmax层之前合并了绝对单词位置嵌入，在该模型中，模型根据词内容和位置的聚合上下文嵌入对Masked单词进行解码`。

此外，我们提出了一种新的虚拟对抗训练方法，用于将PLM微调到下游NLP任务。 该方法可有效改善模型的泛化性能。我们通过全面的实证研究表明，这些技术大大提高了预训练的效率和下游任务的性能。
## 背景
### Transformer
基于transformer的语言模型由堆叠的transformer块组成。每个块都包含一个多头self-attention层，后面是一个全连接的位置前馈网络。标准的self-attention机制缺乏编码单词位置信息的自然方法。因此，现有的方法给每个输入词的嵌入增加了位置偏差，使得每个输入词由一个向量表示，其值取决于其内容和位置。位置偏差可以使用绝对位置嵌入或相对位置嵌入来实现。研究表明，相对位置表示对于自然语言理解和生成任务更有效。提出的解耦注意力机制与所有现有方法的不同之处在于，我们表示每个输入词，通过使用两个对词的内容和位置进行编码的单独向量，并使用解耦的矩阵分别计算其内容和相对位置来计算词间的注意力权重。
### 掩码语言模型
基于transformer的PLM通常在大量文本上进行预训练，称为mask语言模型(MLM)模型，使用自监督目标来学习上下文单词表示。具体来说，给定序列X={x_i},我们通过随机masking其15％的token将其分解为X~，然后训练由θ参数化的语言模型，通过预测以X~为条件的被masking的token x~来重构X：<br>
![](https://img-blog.csdnimg.cn/57a2f0325d0e45f9879271f5b855a8a8.png)<br>
其中C是序列中被mask token的索引集。BERT的作者提议保持10％的mask token不变，另外10％的token由随机选择的token代替，其余的token由[MASK] token代替。
## DEBERTA架构
DEBERTA模型的架构如下图所示：<br>
![](https://img-blog.csdnimg.cn/img_convert/10dc9509df43c25e1c39df95420f9b52.jpeg)<br>
### 解纠缠注意力：内容和位置嵌入的双向量方法
对于BERT模型，输入层的每个单词都使用一个向量来表示，该向量是其单词（内容）嵌入和位置嵌入的总和。
![](https://img-blog.csdnimg.cn/img_convert/2a0edcb5931c094a9770d0b857c59c75.jpeg)
而 DeBERTa 中的每个单词使用两个对其内容和位置分别进行编码的向量来表示，并且注意力单词之间的权重分别使用基于它们的内容和相对位置的解码矩阵来计算。这么做的原因是，经观察发现，单词对的注意力权重不仅取决于它们的内容，还取决于它们的相对位置。 例如，“deep”和“learning”这两个词相邻出现时，它们之间的依赖性比它们出现在不同句子中时要强得多。
对于序列中位置i处的token，我们使用两个向量，{Hi} 和{Pi|j} 表示它，它们分别表示内容和与位置j处的token的相对位置。token i和j之间的交叉注意力得分的计算可以分解为四个部分:<br>
![](https://img-blog.csdnimg.cn/9d0feed3b55d49909cebf8d813cc9aaa.png)<br>
也就是说，**一个单词对的注意力权重可以使用其内容和位置的解耦的矩阵计算为四个注意力(内容到内容，内容到位置，位置到内容和位置到位置)的得分的总和**。<br>
现有的相对位置编码方法在计算注意力权重时使用单独的嵌入矩阵来计算相对位置偏差。这等效于仅使用等式2中的“内容到内容”和“内容到位置”来计算注意力权重。我们认为位置到内容也很重要，因为单词对的注意力权重不仅取决于它们的内容。根据它们的相对位置，只能使用内容到位置和位置到内容进行完全建模。 由于我们使用相对位置嵌入，因此位置到位置项不会提供太多附加信息，因此在我们的实现中将其从等式2中删除。<br>
### 绝对单词位置的增强型mask解码器
DeBERTa是使用MLM进行预训练的，在该模型中，模型被训练为使用mask token周围的单词来预测mask词应该是什么。DeBERTa将上下文的内容和位置信息用于MLM。解耦注意力机制已经考虑了上下文词的内容和相对位置，但没有考虑这些词的绝对位置，这在很多情况下对于预测至关重要。给定一个句子“a new store opened beside the new mall”，并用“store”和“mall”两个词mask以进行预测。 仅使用局部上下文(例如，相对位置和周围的单词)不足以使模型在此句子中区分store和mall，因为两者都以相同的相对位置在new单词之后。 为了解决这个限制，模型需要考虑绝对位置，作为相对位置的补充信息。 例如，句子的主题是“store”而不是“mall”。 这些语法上的细微差别在很大程度上取决于单词在句子中的绝对位置。
有两种合并绝对位置的方法。BERT模型在输入层中合并了绝对位置。**在DeBERTa中，我们在所有Transformer层之后将它们合并，然后在softmax层之前进行mask token预测**，如下图所示。<br>
![](https://img-blog.csdnimg.cn/img_convert/6a06cce1354b40b9dc8b4b6ebad9c66b.jpeg)<br>
EMD有两个输入(即I,H)。H表示来自先前的transformer层的隐藏状态，并且I可以是用于解码的任何必要信息，例如，绝对位置嵌入或从先前的EMD层输出。n表示n个EMD堆叠层，其中每个EMD层的输出将是下一个EMD层的输入I，最后一个EMD层的输出将直接输出到语言模型头。n层可以共享相同的权重。在实验中，我们设定n = 2 ，即2层共享相同的权重，以减少参数的数量，并使用绝对位置嵌入作为第一个EMD层的I。 当 I = H 和 n = 1 时，EMD与BERT解码器层相同。 不过，EMD更通用、更灵活，因为它可以使用各种类型的输入信息进行解码。

这样，DeBERTa捕获了所有Transformer层中的相对位置，仅将绝对位置用作补充信息。因此，我们将DeBERTa的解码组件称为增强型Masked解码器(EMD)。 在实证研究中，我们比较了合并绝对位置的这两种方法，并观察到EMD效果更好。 我们推测，BERT所使用的绝对位置的早期合并可能会不利地阻碍模型学习足够的相对位置信息。此外，EMD还使我们能够引入除位置以外的其他有用信息，以进行预训练。 我们将其留给以后的工作。除NLU任务外，DeBERTa还可以扩展为处理NLG任务。 为了使DeBERTa像自回归模型一样操作以生成文本，我们使用了一个三角形矩阵进行self-attention，并按照Dong(2019)的方法将self-attention Masked的上三角部分设置为负无穷。
## Scale-invariant-Fine-Tuning 不变微调(SiFT)
本节介绍了一种新的虚拟对抗训练算法，Scale-invariant-Fine-Tuning 不变微调(SiFT)，它是Miyato(Jiang et al2020)中描述的算法的一种变体，用于微调。
**虚拟对抗训练是一种改进模型泛化的正则化方法。它通过对抗性样本提高模型的鲁棒性，对抗性样本是通过对输入进行细微扰动而创建的。模型进行正则化，以便在给出特定于任务的样本时，该模型产生的输出分布与该样本的对抗性扰动所产生的输出分布相同**。
对于NLP任务，扰动将应用于单词嵌入，而不是原始单词序列。但是，嵌入向量的value范围(范数)在不同的单词和模型之间有所不同。对于具有数十亿个参数的较大模型，方差会变大，从而导致对抗训练有些不稳定。受层归一化的启发(Ba et al.,2016)，**我们提出了SiFT算法，该算法通过应用扰动的归一化的词嵌入来提高训练稳定性**。 具体来说，`在我们的实验中将DeBERTa微调到下游NLP任务时，SiFT首先将单词嵌入向量归一化为随机向量，然后将扰动应用于归一化的嵌入向量`。我们发现，归一化大大改善了微调模型的性能。 对于较大的DeBERTa模型，此改进更为突出。我们将SiFT的全面研究留给未来的工作。
##  结论
本文提出了一种新的模型架构DeBERTa（解码增强的BERT，具有解缠注意力），它使用两种新技术改进了BERT和RoBERTa模型。第一种是解缠注意力机制，其中每个单词分别使用两个对其内容和位置进行编码的向量表示，并且分别使用解缠矩阵计算单词之间的注意力权重，分别用于其内容和相对位置。第二种是增强的掩码解码器，它在解码层中结合了绝对位置，以预测模型预训练中的掩码令牌。此外，使用一种新的虚拟对抗训练方法进行微调，以提高模型对下游任务的泛化性。我们通过全面的实证研究表明，这些技术显着提高了模型预训练的效率和下游任务的性能。具有 15 亿个参数的 DeBERTa 模型在宏观平均得分方面首次超过了 SuperGLUE 基准测试中的人类表现。DeBERTa在SuperGLUE上超越人类的表现标志着通用AI的一个重要里程碑。尽管在SuperGLUE上取得了令人鼓舞的结果，但该模型绝没有达到NLU的人类水平。人类非常善于利用从不同任务中学到的知识来解决新任务，而没有或很少针对特定任务的演示。这被称为组合泛化，即泛化到熟悉成分（子任务或基本问题解决技能）的新组合（新任务）的能力。展望未来，值得探索的是，如何让DeBERTa以更明确的方式结合组合结构，这可以允许将自然语言的神经和符号计算结合起来，类似于人类所做的事情。
