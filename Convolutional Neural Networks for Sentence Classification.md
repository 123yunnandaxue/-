# Convolutional Neural Networks for Sentence Classification
## 摘要 ##
该篇论文在预先训练的单词向量的基础上，通过训练卷积神经网络（CNN）进行句子级分类任务。向我们展示了一个简单的CNN，它具有强大的超参数调整和静态向量，并且在多个基准测试上取得了优异的结果。实验结果表明通过微调学习特定于任务的向量可以进一步提高性能。论文还对CNN架构进行了简单修改，使其可以用于特定任务和静态向量。本文讨论的模型改进了7个任务中的4个任务的现有技术，这些任务包括情绪分析和问题分类。
## 背景介绍 ##
近年来，深度学习模型在计算机视觉和语音识别方面取得了显著成果。在自然语言处理中，使用深度学习方法的大部分工作都涉及到通过神经语言模型学习单词向量表示，并对学习到的单词向量进行组合以进行分类。单词向量本质上是在其维度上对单词的语义特征进行编码的特征提取器。在这种密集的表示中，语义上接近的词在较低维向量空间中同样接近——在欧几里德或余弦距离上。
卷积神经网络（CNN）利用具有卷积滤波器的层，这些滤波器应用于局部特征。CNN模型最初是为计算机视觉而发明的，后来被证明对NLP有效，并在语义解析、搜索查询检索、句子建模和其他传统的NLP任务中取得了优异的结果。
在目前的工作中，我们训练了一个简单的卷积神经网络，在从无监督神经语言模型中获得的词向量之上有一层卷积。我们最初保持单词向量是静态的，只学习模型的其他参数。尽管对超参数的调整很少，但这个简单的模型在多个基准上取得了优异的结果，这表明预先训练的向量是“通用”的特征提取器，可以用于各种分类任务。通过微调学习特定于任务的向量会带来进一步的改进。最后，我们描述了对架构的一个简单修改，通过具有多个通道，允许使用预先训练的向量和任务特定向量。
## 模型介绍 ##
该篇论文所用的模型是在卷积神经网络架构上的一个轻微变体，模型结构如下图所示。
![Alt text](./屏幕截图 2023-10-12 173250.png)
让一个k维的词向量xi对应于句子中的第i个单词，长度为n的句子可以表示为：
 ![Alt text](./屏幕截图 2023-10-12 190218.png)
一个卷积包含一个过滤器w，其被应用于h个单词的窗口以产生新的特征，例如特征ci的产生公式为：![Alt text](./屏幕截图 2023-10-12 190531.png)， 这里的b是系统偏差，f是非线性函数。这个过滤器会被用于句子中的每一个窗口，从而得到一个特征映射![Alt text](./屏幕截图 2023-10-12 191009.png)。然后，在特征映射上应用最大随时间池化操作，并将特征映射中的最大值作为对应于该特定滤波器的特征。其想法是为每个特征图捕获最重要的特征——具有最大值的特征。该模型在其他方面等效于单通道架构。
## 正则化
对于正则化，我们在倒数第二层使用Dropout，并对权重向量的L2范数进行约束。Dropout通过在反向传播过程中随机丢弃（即设置为零）隐藏单元的比例p来防止隐藏单元的协同适应。也就是说，给予倒数第二层过滤器，对于正向传播输出单元y使用![Alt text](./屏幕截图 2023-10-12 193459.png)。其中o是逐元素乘法算子，r是概率p为1的伯努利随机变量的“掩蔽”向量。梯度仅通过未屏蔽的单元进行反向传播。在测试是，已学习的权重向量乘以一个p进行缩放，如：![Alt text](./屏幕截图 2023-10-12 193852.png) ，其中w^被用于评估无法被看见的句子。并且额外限制了权重向量的L2范数，使得w的L2范数等于s，在一个梯度下降步骤之后，w的L2范数无论何时都大于s。
## 数据集和实验设置
论文在各种基准上进行模型测试，共使用了7个数据集。
### 超参数和预训练
对于所有的数据集，通过网格搜素法进行超参数调整，除了在dev数据集上提前停止之外，不执行任何特定于数据集的调优。对于没有标准dev集的数据集，我们随机选择10%的训练数据作为dev集。训练是通过使用Adadelta更新规则在小批量上的随机梯度下降来完成的。
### 预训练词向量
在没有大型监督训练集的情况下，用从无监督神经语言模型中获得的词向量来初始化词向量是提高性能的一种流行方法。我们使用公开可用的word2vec向量，这些向量是在谷歌新闻的1000亿个单词上训练的。向量的维度为300，并使用连续词袋结构进行训练。预训练单词集合中不存在的单词被随机初始化。
### 模型变化
论文采用了几个CNN模型的变体进行测试（CNN-rand、CNN-static、CNN-non-static、CNN-multichannel）,为了区分上述变化与其他随机因素的影响，我们通过在每个数据集中保持一致，消除了其他随机性来源。
| Model     |introduction  | 
| :---------| --------| 
| CNN-rand  |单词被随机初始化，然后在训练期间进行调整 |  
| CNN-static| 一个具有来自word2vec的预训练向量的模型所有单词——包括随机初始化的未知单词——都保持静态，并且只学习模型的其他参数。 |  
| CNN-non-static| 与CNN-static相同，但预训练的向量会针对每个任务进行微调 | 
|CNN-multichannel|一个包含两组单词向量的模型。每组向量被视为一个“通道”，每个滤波器都应用于两个通道，但梯度仅通过其中一个通道反向传播。因此，该模型能够微调一组向量，同时保持另一组向量不变。两个通道都用word2vec初始化。|  
## 结果与讨论
下表中列出了我们的模型与其他方法的对比结果。
![Alt text](./屏幕截图 2023-10-12 212805.png)
从表中可以看出，CNN rand模型的表现不佳。虽然我们期望通过使用预先训练的向量来获得性能增益，但增益的幅度很低。即使是具有静态向量的简单模型（CNN-static）也表现得非常好，与使用复杂池化方案或需要预先计算解析树的更复杂的深度学习模型相比，也能给出有竞争力的结果。这些结果表明，预训练的向量是良好的“通用”特征提取器，可以在数据集中使用。对每个任务的预训练向量进行微调可以得到进一步的改进（CNN-non-static）。
### 多通道和单通道的对比
我们最初希望多通道架构能够防止过拟合（通过确保学习的向量不会偏离原始值太远），从而比单通道模型更好地工作，尤其是在较小的数据集上。然而，结果喜忧参半，有必要进一步规范微调过程。例如，与其对非静态部分使用额外的通道，不如直接保持单个通道，但允许在训练期间额外的维度被修改。
### 静态与非静态模型表示的对比
与单通道非静态模型的情况一样，多通道模型能够微调非静态通道，使其更适合手头的任务。例如，‘good’在word2vec中与bad最相似，可能是因为它们在语法上（几乎）是等价的。但对于在SST-2数据集上微调的非静态通道中的矢量，情况不再如此。同样，在表达情感方面，‘好’可以说更接近于‘好’，而不是‘伟大’，这确实反映在习得的向量中。对于不在预先训练的向量集中的（随机初始化的）标记，微调可以让它们学习更有意义的表示：网络学习到感叹号与溢美的表达式相关联，逗号是连词。
### 进一步观察
我们报告了一些进一步的实验和观察结果：  
•Kalchbrenner等人（2014）报告了CNN的更差结果，该CNN的架构与我们的单通道模型基本相同。例如，他们的具有随机初始化单词的最大TDNN（时延神经网络）在SST-1数据集上获得了37.4%，而我们的模型获得了45.0%。我们将这种差异归因于我们的CNN具有更大的容量（多个滤波器宽度和特征图）。  
•Dropout被证明是一个很好的正则化器，因此使用一个比必要的网络更大的网络并简单地让Dropout正则化是可以的。Dropout始终增加2%-4%的相对性能。
•当随机初始化不在word2vec中的单词时，我们通过从U[−a，a]中的每个维度采样获得了轻微的改进，其中选择了a，使得随机初始化的向量与预训练的向量具有相同的方差。有趣的是，在初始化过程中采用更复杂的方法来反映预先训练的向量的分布是否会带来进一步的改进。  
•我们在维基百科上对Collobert等人（2011）训练的另一组公开可用的单词向量进行了简短的实验，发现word2vec的性能要高得多。目前尚不清楚这是由于Mikolov等人（2013）的架构还是1000亿字的谷歌新闻数据集。  
•Adadelta（Zeiler，2012）给出了与 Adagrad（Duchi et al.，2011）相似的结果，但只需要更少的时间。  
## 结论
在目前的工作中，我们描述了一系列基于word2vec的卷积神经网络实验。尽管超参数几乎没有调整，但具有一层卷积的简单CNN表现得非常好。我们的研究结果进一步证明了单词向量的无监督预训练是NLP深度学习的重要组成部分。
