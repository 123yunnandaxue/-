# Sequence to Sequence Learning with Neural Networks
## 摘要
深度神经网络（DNN）是一种强大的模型，在困难的学习任务中取得了优异的性能。**尽管DNN在大型标记训练集可用时工作良好，但它们不能用于将序列映射到序列**。在本文中，**我们提出了一种通用的端到端序列学习方法，该方法对序列结构进行最小假设**。**我们的方法使用多层长短期记忆（LSTM）将输入序列映射到固定维度的向量，然后使用另一个深度LSTM从向量中解码目标序列**。我们的主要结果是，在WMT’14数据集的英法翻译任务中，LSTM产生的翻译在整个测试集上的BLEU得分为34.8，其中LSTM的BLEU分数在词汇表外单词上受到惩罚。此外，LSTM在长句方面没有困难。相比之下，基于短语的SMT系统在同一数据集上获得了33.3的BLEU分数。当我们使用LSTM对上述SMT系统产生的1000个假设进行重新排序时，其BLEU得分增加到36.5，这接近于该任务之前的最佳结果。**LSTM还学习了对语序敏感且对主动语态和被动语态相对不变的合理短语和句子表示**。最后，我们发现，**颠倒所有源句子（而不是目标句子）中的单词顺序显著提高了LSTM的性能，因为这样做在源句子和目标句子之间引入了许多短期依赖关系**，这使得优化问题变得更容易。
## 介绍
* `深度神经网络（DNN）的优点`：是一种功能强大的机器学习模型，在语音识别和视觉对象识别等难题上表现出色。DNN功能强大，因为它们**可以执行任意的并行计算**，只需少量的步骤。只要标记的训练集具有足够的信息来指定网络的参数，就可以使用监督反向传播来训练大型DNN。因此，**如果存在实现良好结果的大型DNN的参数设置，则监督反向传播将找到这些参数并解决问题**。
* `深度神经网络（DNN）的限制`：**尽管DNN具有灵活性和强大的功能，但它只能应用于输入和目标可以用固定维度的向量进行合理编码的问题**。这是一个显著的限制，因为许多重要的问题最好用长度不是先验已知的序列来表达。因此，很明显，学习将序列映射到序列的领域独立方法将是有用的。

序列对DNN提出了挑战，因为它们要求输入和输出的维度是已知的和固定的。**在本文中，我们证明了长短期存储器（LSTM）架构的直接应用可以解决一般的序列到序列问题**。**其思想是使用一个LSTM来读取输入序列，一次一个时间步长，以获得大的固定维向量表示，然后使用另一个LSTM从该向量中提取输出序列。第二个LSTM本质上是一个递归神经网络语言模型，只是它以输入序列为条件**。LSTM在具有长范围时间依赖性的数据上成功学习的能力使其成为该应用程序的自然选择，因为输入和相应输出之间存在相当大的时间滞后(如下图所示)。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/01.png)

`我们的主要成果如下`：
* 在WMT’14英法翻译任务中，我们通过使用简单的从左到右波束搜索解码器从5个深度LSTM的集合（每个具有384M个参数和8000维状态）中直接提取翻译，获得了34.81的BLEU分数。这是迄今为止使用大型神经网络进行直接翻译所获得的最佳结果。相比之下，SMT基线在该数据集上的BLEU得分为33.30。34.81 BLEU得分是由词汇量为80k个单词的LSTM获得的，因此每当参考翻译包含这80k个词之外的单词时，该得分就会受到惩罚。**这一结果表明，一个相对未优化的小词汇神经网络体系结构比基于短语的SMT系统有很大的改进空间**。
*  我们使用LSTM对相同任务的SMT基线的公开可用的1000个最佳列表进行重新排序。通过这样做，我们获得了36.5的BLEU得分，这将基线提高了3.2个BLEU分，接近该任务之前公布的最佳结果（37.0）。

令人惊讶的是，尽管其他研究人员最近有相关架构的经验，LSTM并没有受到很长句子的影响。**我们能够在长句上做得很好，因为我们在训练和测试集中颠倒了源句中的单词顺序，但没有颠倒目标句的顺序**。**通过这样做，我们引入了许多短期依赖关系，使优化问题更加简单**。因此，SGD可以学习LSTM，而不会遇到长句问题。颠倒源句中单词的简单技巧是这项工作的关键技术贡献之一。
**LSTM的一个有用特性是，它学习将可变长度的输入句子映射到固定维的向量表示中**。鉴于翻译往往是对源句的转述，翻译目标鼓励LSTM找到能够捕捉其含义的句子表征，因为具有相似含义的句子彼此接近，而不同的句子含义则相距遥远。定性评估支持了这一说法，**表明我们的模型了解语序，并且对主动语态和被动语态相对不变**。
## 模型
循环神经网络（RNN）是前馈神经网络对序列的自然推广。给定输入序列（x1，…，xT），标准RNN通过迭代以下等式来计算输出序列（y1，…，yT）：<br>
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/02.png)<br>
只要输入和输出之间的对齐提前已知，RNN就可以容易地将序列映射到序列。然而，如何将RNN应用于输入和输出序列具有不同长度、具有复杂和非单调关系的问题尚不清楚。一般序列学习的最简单策略是使用一个RNN将输入序列映射到固定大小的向量，然后使用另一个RNN-将向量映射到目标序列（Cho等人也采用了这种方法）。虽然原则上可以工作，因为RNN提供了所有相关信息，由于所产生的长期依赖性，训练RNN将是困难的。然而，众所周知，长短期记忆（LSTM）会学习具有长范围时间依赖性的问题，因此LSTM在这种情况下可能会成功。
LSTM的目标是估计条件概率p（y1，…，yT′|x1，…，xT），其中（x1，yT′）是其相应的输出序列，其长度T′可能不同于T。LSTM通过首先获得由LSTM的最后一个隐藏状态给出的输入序列（x1，…，xT）的固定维表示v，然后计算y1...yT′具有其初始隐藏状态被设置为x1...xT的表示v的标准LSTM-LM公式：<br>
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/03.png)<br>
在这个等式中，每个p（yt|v，y1，…，yt−1）分布用词汇表中所有单词的softmax表示。我们使用Graves中的LSTM公式。**请注意，我们要求每个句子以一个特殊的句子结尾符号“<EOS\>”结束**，这使模型能够定义所有可能长度的序列上的分布。总体方案如图1所示，其中所示的LSTM计算“A”、“B”、“C”、“<EOS\>”的表示，然后使用该表示来计算“W”、“X”、“Y”、“Z”、“<EOS>“的概率。

我们的实际模型在三个重要方面与上述描述不同：
* 我们**使用了两种不同的LSTM**：一种用于输入序列，另一种用于输出序列，因为这样做会以可忽略的计算成本增加数字模型参数，并使同时在多个语言对上训练LSTM变得很自然。
*  我们发现**深层LSTM显著优于浅层LSTM**，因此我们选择了具有四层的LSTM。
*  我们发现**颠倒输入句子的单词顺序是非常有价值的**。因此，例如，LSTM被要求将c、b、a映射到α、β、γ，而不是将句子a、b、c映射到句子α、β和γ，其中α、β，γ是a、b和c的翻译。这样，a与α非常接近，b与β非常接近，依此类推，这一事实使SGD很容易在输入和输出之间“建立通信”。我们发现这种简单的数据转换可以极大地提高LSTM的性能。
## 实验
### 颠倒源句子
**虽然LSTM能够解决具有长期依赖性的问题，但我们发现，当源句子颠倒（目标句子不颠倒）时，LSTM学习得更好**。虽然我们对这一现象没有完整的解释，但**我们认为这是由于数据集引入了许多短期依赖性造成的**。通常，`当我们将源句子与目标句子连接时，源句子中的每个单词都与目标句子中的对应单词相距甚远。因此，该问题具有较大的“最小时滞”。通过颠倒源句子中的单词，源语言和目标语言中对应单词之间的平均距离不变。然而，源语言中的前几个单词现在与目标语言中的头几个单词非常接近，因此大大减少了问题的最小时滞。因此，反向传播在源句子和目标句子之间更容易“建立通信”，这反过来又大大提高了整体性能。`
最初，我们认为，颠倒输入句子只会在目标句子的早期部分产生更自信的预测，而在后期部分产生不那么自信的预测。然而，与在原始源句上训练的LSTM相比，在反向源句上培训的LSTM在长句上表现得更好，**这表明反向输入句会导致LSTM具有更好的记忆利用率。**
### 实验结果
结果见表1和表2。我们的最佳结果是用LSTM的集合获得的，这些LSTM在随机初始化和小批量的随机顺序方面不同。虽然LSTM集合的解码翻译并不优于最好的WMT’14系统，但这是纯神经翻译系统首次在大规模MT任务中以相当大的优势优于基于短语的SMT基线，尽管它无法处理词汇表外的单词。如果LSTM用于重新存储基线系统的1000个最佳列表，则它与最佳WMT’14结果的BLEU点在0.5以内。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/04.png)
### 在长句子上的表现
我们惊讶地发现，LSTM在长句上表现良好，如下图所示。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/05.png)
### 模型分析
我们的模型的一个吸引人的特点是它**能够将单词序列转换为固定维度的向量**。下图显示了一些学习到的表示。该图清楚地表明，表征对单词的顺序很敏感，而对用被动语态替换主动语态则相当不敏感。
![](https://github.com/123yunnandaxue/paper-notebook/blob/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/picture/06.png)
## 总结
在这项工作中，我们表明，**具有有限词汇表并且几乎不对问题结构进行假设的大型深层LSTM，在大规模MT任务中可以优于词汇表不受限制的标准基于SMT的系统**。**我们基于MT的简单LSTM方法的成功表明，只要它们有足够的训练数据，它应该在许多其他序列学习问题上做得很好**。
我们对颠倒源句中的单词所获得的改善程度感到惊讶。我们得出的结论是，**找到一个具有最多短期依赖性的问题编码是很重要的，因为它们使学习问题变得更简单**。特别是，虽然我们无法在非反向翻译问题上训练标准RNN，但我们认为，当源句反向时，标准RNN应该很容易训练（尽管我们没有通过实验验证）。
我们还对LSTM正确翻译超长句子的能力感到惊讶。我们最初确信，由于LSTM的记忆力有限，它在长句上会失败，其他研究人员报告说，使用与我们类似的模型，它在长句上的表现较差。然而，**在反向数据集上训练的LSTM在翻译长句方面几乎没有困难**。
最重要的是，我们证明了一种简单、直接且相对未优化的方法可以胜过SMT系统，因此进一步的工作可能会带来更大的翻译准确度。这些结果表明，我们的方法可能会在其他具有挑战性的序列到序列问题上做得很好。



