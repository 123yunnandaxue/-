# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
## 摘要
本文提出了BERT，它是基于Transformer的双向编码表示。它与之前的语言表示模型不一样，主要列举了两篇文章进行对比：
* ELMO（Embeddings from Language Models，2018）模型，使用双向编码的词向量表征模型，采用的是LSTM架构。
* GPT（Generative Pre-trained Transformer，2018年OPENAI基于transformer提出），是基于transformer的单向语言模型，用左侧的上下文信息预测未来。

**BERT通过联合调节所有层中的左右上下文信息来预训练未标记文本的深度双向表示**。因此，**预训练的 BERT 模型可以通过一个额外的输出层进行微调，从而适用于NLP领域中的很多任务，并取得较好的效果**，例如问答和语言推理，同时无需对特定任务的架构进行大量修改。
## 介绍
>所谓的“预训练”，其实并不是什么新概念，这种“Pre-training and Fine-tuning”的方法在图像领域早有应用。2009年，邓嘉、李飞飞等人在CVPR 2009发布了ImageNet数据集，其中120万张图像分为1000个类别。基于ImageNet，以图像分类为目标使用深度卷积神经网络（如常见的ResNet、VCG、Inception等）进行预训练，得到的模型称为预训练模型。针对目标检测或者语义分割等任务，基于这些预训练模型，通过一组新的全连接层与预训练模型进行拼接，利用少量标注数据进行微调，将预训练模型学习到的图像分类能力迁移到新的目标任务。预训练的方式在图像领域取得了广泛的成功，比如有学者将ImageNet上学习得到的特征表示用于PSACAL VOC上的物体检测，将检测率提高了20%。
图像领域预训练的成功也启发了NLP领域研究，深度学习时代广泛使用的词向量（即词嵌入，Word Embedding）即属于NLP预训练工作。使用深度神经网络进行NLP模型训练时，首先需要将待处理文本转为词向量作为神经网络输入，词向量的效果会影响到最后模型效果。词向量的效果主要取决于训练语料的大小，很多NLP任务中有限的标注语料不足以训练出足够好的词向量，通常使用跟当前任务无关的大规模未标注语料进行词向量预训练，因此预训练的另一个好处是能增强模型的泛化能力。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2Vec和GloVe等）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。<br>

语言模型预训练已被证明可以有效改善许多自然语言处理任务。这些包括句子级别的任务，如自然语言推理和释义，旨在通过整体分析句子来预测句子之间的关系，以及标记级任务，例如命名实体识别和问答，其中模型需要在标记级产生细粒度输出。<br>
>预训练词向量通常只编码词汇间的关系，对上下文信息考虑不足，且无法处理一词多义问题。如“bank”一词，根据上下文语境不同，可能表示“银行”，也可能表示“岸边”，却对应相同的词向量，这样显然是不合理的。为了更好的考虑单词的上下文信息，Context2Vec使用两个双向长短时记忆网络（Long Short Term Memory，LSTM）来分别编码每个单词左到右（Left-to-Right）和右到左（Right-to-Left）的上下文信息。类似地，ELMo也是基于大量文本训练深层双向LSTM网络结构的语言模型。ELMo在词向量的学习中考虑深层网络不同层的信息，并加入到单词的最终Embedding表示中，在多个NLP任务中取得了提升。ELMo这种使用预训练语言模型的词向量作为特征输入到下游目标任务中，被称为Feature-based方法。另一种方法是微调（Fine-tuning）。GPT（只考虑单向的语义信息）、BERT和后续的预训练工作都属于这一范畴，直接在深层Transformer网络上进行语言模型训练，收敛后针对下游目标任务进行微调，不需要再为目标任务设计Task-specific网络从头训练。

**将预训练的语言表示应用于下游任务有两种现有策略：基于特征和微调**。基于特征的方法，例如ELMo，使用特定于任务的架构，其中包括预训练的表示作为附加功能。微调方法，例如生成式预训练转换器 （OpenAI GPT），引入了最少的任务特定参数，并通过简单地微调所有预训练参数来训练下游任务。这两种方法在预训练期间共享相同的目标函数，它们使用单向语言模型来学习一般语言表示。<br>
**我们认为，当前的技术限制了预训练表示的能力，特别是对于微调方法。主要限制是标准语言模型是单向的，这限制了在预训练期间可以使用的体系结构的选择**。例如，在 OpenAI GPT 中，作者使用从左到右的架构，其中每个令牌只能关注 Transformer 自注意力层中的先前令牌。这种限制对于句子级任务来说是次优的，并且在将基于微调的方法应用于标记级任务（如问答）时可能非常有害，因为在这些任务中，从两个方向合并上下文至关重要。<br>
**在本文中，我们通过提出 BERT：来自 Transformer 的双向编码器表示来改进基于微调的方法。BERT通过使用“掩码语言模型”（MLM）预训练目标来缓解前面提到的单向性约束**，该目标的灵感来自Cloze任务。**掩码语言模型随机屏蔽输入中的一些标记，目的是仅根据其上下文预测被掩蔽词的原始词汇 ID**。与从左到右的语言模型预训练不同，MLM 目标使表示能够融合左右上下文，这使我们能够预训练深度双向 Transformer。除了掩码语言模型之外，我们还使用“下一句话预测”任务来联合预训练文本对表示。本文的贡献如下：
* 我们**证明了双向预训练对语言表征的重要性**。与Radford等人（2018）使用单向语言模型进行预训练不同，BERT使用掩码语言模型来实现预训练的深度双向表示。这也与Peters等人（2018a）形成鲜明对比，后者使用独立训练的从左到右和从右到左LM的浅层串联。
* 我们表明，**预训练的表示减少了对许多经过大量设计的特定任务架构的需求。BERT 是第一个基于微调的表示模型**，可在大量句子级和标记级任务上实现最先进的性能，优于许多特定于任务的架构。
## 相关工作
预训练通用语言表示由来已久，我们在本节中简要回顾了最广泛使用的方法。
### 基于无监督特征的方法
为了训练句子表征，以前的工作使用目标对候选的下一个句子进行排名（Jernite et al.， 2017;Logeswaran 和 Lee，2018 年），给定前一个句子的表示，从左到右生成下一个句子单词（Kiros 等人，2015 年），或去噪自动编码器衍生的目标（Hill 等人，2016 年）。ELMo 及其前身 （Peters et al.， 2017， 2018a） 将传统的词嵌入研究沿不同维度进行推广。它们从从左到右和从右到左的语言模型中提取上下文相关特征。每个标记的上下文表示是从左到右和从右到左表示的串联。当将上下文词嵌入与现有的特定于任务的架构集成时，ELMo 推进了几个主要 NLP 基准（Peters 等人，2018a）的最新技术，包括问答（Rajpurkar 等人，2016 年）、情感分析（Socher 等人，2013 年）和命名实体识别（Tjong Kim Sang 和 De Meulder，2003 年）。Melamud et al. （2016） 提出通过一项任务来学习上下文表示，以使用 LSTM 从左侧和右侧上下文中预测单个单词。 与 ELMo 类似，他们的模型是基于特征的，而不是深度双向的。Fedus et al. （2018） 表明，完形填空任务可用于提高文本生成模型的鲁棒性。
### 无监督微调方法
与基于特征的方法一样，第一种方法只对来自未标记文本的预训练词嵌入参数起作用（Collobert and Weston，2008）。最近，产生上下文标记表示的句子或文档编码器已经从未标记的文本中进行了预训练，并针对受监督的下游任务进行了微调（Dai and Le，2015;Howard 和 Ruder，2018 年;Radford等人，2018）。这些方法的优点是需要从头开始学习的参数很少。至少部分由于这一优势，OpenAI GPT（Radford 等人，2018 年）在 GLUE 基准测试（Wang 等人，2018 年）的许多句子级任务上取得了以前最先进的结果。
## BERT
我们在本节中介绍 BERT 及其详细实现。我们的框架中有两个步骤：预训练和微调。**在预训练期间，模型在不同的预训练任务中对未标记的数据进行训练。对于微调，首先使用预先训练的参数初始化 BERT 模型，然后使用来自下游任务的标记数据对所有参数进行微调直至收敛。每个下游任务都有单独的微调模型，即使它们使用相同的预训练参数进行初始化**。如下图所示：<br>
![](https://img-blog.csdnimg.cn/ac1b6e780c854caa90f457c2a9c96479.png)<br>
预训练模型架构的差异如下图所示。BERT使用双向Transformer。OpenAI GPT使用从左到右的Transformer。ELMo使用独立训练的从左到右和从右到左lstm的连接来为下游任务生成特征。
在这三种表示中，只有BERT表示同时以所有层中的左右上下文为条件。除了架构差异之外，BERT和OpenAI GPT是一种Fine-tuning方法，而ELMo是一种Feature-based的方法。<br>
![](https://img-blog.csdnimg.cn/5d4eeddcdc5c4e428d67e474cc69be09.png)<br>
**输入输出表示**：我们使用带有30000个token词汇表的WordPiece embeddings。每个序列的第一个标记总是一个特殊的分类标记([CLS])。
句子对被打包成一个序列。我们用两种方法来区分这些句子。首先，我们用一个特殊的标记([SEP])将它们分开。其次，我们为每个标记添加一个学习嵌入，表明它属于句子a还是句子b。<br>
**BERT模型采用了Transformer中的Encoder架构，通过引入多头注意力机制，将Encoder块进行堆叠，形成最终的BERT架构**。通过使用Transformer作为模型的主要框架，BERT能够更彻底地捕获语句中的双向关系，极大地提升了预训练模型在具体任务中的性能。
BERT 模型的输入由三部分组成。除了传统意义上的 token 词向量外，**BERT 还引入了位置词向量和句子词向量。位置词向量的思想与 Transformer 一致，但 BERT 并未使用其计算公式，而是随机初始化后放入模型一同训练；句子词向量实质上是一个0-1表征，目的是区分输入段落中的上下句**。这三种不同意义的词向量相加，构成了最终输入模型的词向量。
![](https://pic2.zhimg.com/v2-48728982d60e16908ea72ebc6dd58f2d_r.jpg)
### 预训练BERT
与Peters et al. （2018a）和Radford et al. （2018）不同，我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。取而代之的是，我们使用本节所述的两个无监督任务来预训练 BERT。BERT的预训练（pre-training）部分使用了完形填空和上下句匹配两项无监督任务。
1. “完形填空”代表了词语级别的预训练任务，该任务对输入句子中若干随机位置的字符进行遮盖，并利用上下文语境对遮盖字符进行预测。这样做会产生两个缺点：
    * 会造成预训练和微调时的不一致，因为在微调时[MASK]总是不可见的；解决办法是：把80%需要被替换成[MASK]的词进行替换，10%的随机替换为其他词，10%保留原词。由于Transformer Encoder并不知道哪个词需要被预测，哪个词是被随机替换的，这样就强迫每个词的表达需要参照上下文信息。
    * 由于每个Batch中只有15%的词会被预测，因此模型的收敛速度比起单向的语言模型会慢，训练花费的时间会更长。对于第二个缺点目前没有有效的解决办法，但是从提升收益的角度来看，付出的代价是值得的。

例如：
* 80% 为特殊的“ <mask>” 词元（例如，“ this movie is great ”变为“ this movie is<mask> ”）；
* 10%为随机替换词元（例如，“ this movie is great ”变为“ this movie is drink ”）；
* 10%为不变的标签词元（例如，“ this movie is great ”变为“ this movie is great ”）。

2. “上下句匹配”代表了句子级别的预训练任务，该任务给出两个句子，利用句子之间的语义连贯性判定这两个句子是否存在上下句关系。这两项预训练任务对于大量NLP任务的架构具有更好的代表性，同时也更能匹配模型本身的双向架构，对模型的泛化能力有着巨大的提升帮助。为了训练一个理解句子间关系的模型，引入一个下一句预测任务。这一任务的训练语料可以从语料库中抽取句子对包括两个句子A和B来进行生成，其中50%的概率B是A的下一个句子，50%的概率B是语料中的一个随机句子。NSP任务预测B是否是A的下一句。NSP的目的是获取句子间的信息，这点是语言模型无法直接捕捉的。Google的论文结果表明，这个简单的任务对问答和自然语言推理任务十分有益，但是后续一些新的研究发现，去掉NSP任务之后模型效果没有下降甚至还有提升。我们在预训练过程中也发现NSP任务的准确率经过1-2个Epoch训练后就能达到98%-99%，去掉NSP任务之后对模型效果并不会有太大的影响。
### 微调BERT
训练具体任务时，我们只需将具体任务中的输入输出传入预训练完成的 BERT 模型，继续调整参数直至模型再次收敛。该过程称为微调（fine-tuning）。相比于预训练来说，微调的代价是极小的。在大部分NLP任务中，我们只需要在GPU上对模型进行几个小时的微调，便可使模型在具体任务上收敛，完成训练。
## 实验分析与结论
### 实验设置
本文将BERT模型迁移至11个NLP基准任务上进行了微调训练，均取得了SOTA的效果。另外，为了探究模型的不同组成部分对整体性能的影响，本文还进行了若干消融实验，对BERT的预训练任务、模型规模等要素进行了实验评估，充分论证了双向模型的重要性。
### 消融实验及结果分析
本部分对BERT模型的多个部分进行了消融实验研究，旨在探寻它们对于整体模型的重要程度。
#### 预训练任务
本部分通过对 BERT 预训练任务进行消融，旨在论证 BERT 深度双向模型这一创新思想的重要性。实验共设置了两组消融，其中一组使用双向完形填空任务但不使用上下句预测任务，另一组同样不使用上下句预测任务，但实现完形填空任务时采用从左到右的标准模型。文章首先探究了上下句预测任务的取消带来的影响，发现其严重降低了 QNLI，MNLI 和 SQuAD 1.1 这三个任务的性能。其次，通过改变完形填空任务的训练方式，来探究双向训练带来的影响。结果表明，在所有任务上，从左到右的单向模型性能都收获了更差的效果，在 MRPC 和 SQuAD 这两个任务上尤为显著。
#### 模型规模
本部分旨在探究模型大小对微调任务准确度的影响。实验设置了若干具有不同层数、隐层维度以及注意力头数目的模型，并在 GLUE 数据集上进行了微调训练。结果表明，即使是在有标签数据量较小的数据集上，随着模型规模的提高，任务的准确度都获得了显著的提升。此实验进一步论证了，如果模型已经经过了充分的预训练，那么当将模型缩放到一个极限的规模尺寸时，仍然能够在小规模的微调任务上产生较大的改进。
## 结论
最近由于语言模型的迁移学习而进行的实证改进表明，丰富的、无监督的预训练是许多语言理解系统不可或缺的一部分。特别是，这些结果使资源不足的任务也能从深度单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构中，使相同的预训练模型能够成功处理广泛的NLP任务。


