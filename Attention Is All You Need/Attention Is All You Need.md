# Attention Is All You Need
## 摘要
当前主要的序列转换模型基于编码器-解码器配置中的复杂递归或卷积神经网络。表现最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即Transformer，它完全基于注意力机制，完全消除了递归和卷积。我们通过成功地将其应用在具有大量训练数据和有限训练数据的英语选区解析任务上，展示了Transformer能够很好地推广到其他任务。<br>
**总结**：<br>
1. Tranformer的优点：
* 靠attention机制，不使用rnn和cnn，并行度高
* 通过attention，抓长距离依赖关系比rnn强<br>
2. Tranformer的创新点：<br>
* 通过self-attention，自己和自己做attention，使得每个词都有全局的语义信息
* 由于 Self-Attention 是每个词和所有词都要计算 Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。
* 提出multi-head attention，可以看成attention的ensemble版本，不同head学习不同的子空间语义。
## 介绍
1. 循环神经网络，特别是长短期记忆和门控循环神经网络，但是循环神经网络的缺点在于：无法实现训练样例内的并行化。最近的工作通过分解技巧和条件计算在计算效率上取得了显著提高，同时在后者的情况下还提高了模型性能。 但是，顺序计算的基本约束仍然存在
2.  注意力机制已成为各种任务中引人注目的序列建模和转换模型不可或缺的一部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。 但是，除了少数情况外，在所有情况下，此类注意机制都与循环网络结合使用。

鉴于循环神经网络的缺点，在这项工作中，我们提出了Transformer，它是一种避开循环网络的模型体系结构，并且完全依赖于注意力机制来绘制输入和输出之间的全局依存关系。 在八个P100 GPU上进行了仅仅12个小时的训练之后，所提出的Transformer可以实现更多的并行化，并可以在翻译质量方面达到新的最先进水平。
## 背景
减少序列计算的目标也构成了扩展神经GPU ，ByteNet 和ConvS2S 的基础，它们全部使用卷积神经网络作为基本构件，并行计算所有输入和输出位置的隐藏表示。 **在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数在位置之间的距离中增加**，对于ConvS2S是线性增长，而对于ByteNet则是对数增长。 这使得学习远距离之间的依存关系变得更加困难。 **在Transformer中，此操作被减少为常数的操作次数，尽管由于平均注意力加权位置而导致有效分辨率降低的代价，我们用多头注意力来抵消这种影响**。
**自我注意力（有时称为内部注意力）是一种与单个序列的不同位置相关的注意力机制，目的是计算序列的表示形式**。 自我注意力已成功用于各种任务中，包括阅读理解，抽象性摘要，文本蕴涵和学习与任务无关的句子表示。端到端记忆网络基于递归注意力机制，而不是序列对齐的递归，并且已被证明在简单语言问答和语言建模任务中表现良好。据我们所知，**Transformer是第一个完全依靠自我注意力来计算其输入和输出表示的转换模型，而无需使用序列对齐的RNN或卷积**。
## 模型架构
大多数竞争性神经序列转换模型都具有编码器-解码器结构，Transformer遵循此总体架构，对编码器和解码器使用堆叠式的自注意力和逐点，全连接层，分别如下图的左半部分和右半部分所示。<br>
![](https://pic1.zhimg.com/80/v2-ce78b7bdd33d8cadd07d8881a1a19334_720w.webp)<br>
上图可以细化为：
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxWYq8JwSCSojibKvib3yxlYABAMfPZV8fEmtf7kctORRBkMau93Nib6PpXqKXgpUibWr4ygcHzKhPYOVQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
### 编码和解码堆栈
1. 编码器：编码器由N = 6个相同层的叠加组成。每层都有两个子层（如上图的左半图所示）。第一个是多头自我注意力机制，第二个是简单的位置全连接前馈网络。我们在两个子层的每一层周围都采用了残差连接，然后进行层归一化（图中的Add&Norm）。
2. 解码器：解码器还由N = 6个相同层的堆栈组成。 除了每个编码器层中的两个子层之外，解码器还插入第三子层，该第三子层对编码器堆栈的输出执行多头关注。 与编码器类似，我们在每个子层周围采用残余连接，然后进行层归一化。 我们还修改了解码器堆栈中的自我注意子层，以防止位置关注后续位置。这种掩盖，加上输出编码被一个位置偏移的事实，确保了对位置i的预测只能依赖于小于i位置的已知输出。
### 注意力
注意力函数可以描述为将一个查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出计算为值的加权总和，其中分配给每个值的权重由具有相应键的查询的兼容性函数计算。<br>
![](image.png)<br>
首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。
1. 计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说**对于每个单词，我们创造一个查询向量、一个键向量和一个值向量**。**这三个向量是通过词嵌入与三个权重矩阵后相乘创建的**。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJZR0rAD3otWvlHAbLAcsy0MicF0EtW40YlLGdEoueia3njJPdDAv3xMvQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
X1与WQ权重矩阵相乘得到q1, 就是与这个单词相关的查询向量。最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。
1. 计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJHtiaXG83WquVTiakFIribbroawrEYv2d11USCVibzBy11zDZQwwz5g80ibg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
1. 第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJU4nu550MWCiaibZCgPMDUxXlIYAsgOWdg1VZpWK7ajantq5SU8USd8FA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
1. 第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。
2. 第六步是对加权值向量求和（自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。这样自自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJKPJ1YQqGAqsMrFVuQwq8uDPLskVIGOlewK72BCrUsA3cCaqSsHL4Og/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
#### Scaled Dot-Product Attention
我们称特别注意为“缩放点积注意”。<br>
![](https://pic1.zhimg.com/80/v2-30db0b711397b078dadb327373e1f588_720w.webp)
最常用的两个注意函数是加法注意力和乘法注意力（多重复合注意）。乘法注意力与我们的算法相同，除了比例因子。乘法注意力就是用乘法来计算attention score,公式如下所示。<br>
![](https://pic2.zhimg.com/80/v2-843a789df859d080bfd09eee7fca989d_720w.webp)<br>
乘法注意力不用使用一个全连接层，所以空间复杂度占优；另外由于乘法可以使用优化的矩阵乘法运算，所以计算上也一般占优。<br>
论文中的乘法注意力除了一个scale factor:<br>
![](https://pic2.zhimg.com/80/v2-0f3f6a5622baf7ea8d9749f533022679_720w.webp)<br>
论文中指出当dk比较小的时候，乘法注意力和加法注意力效果差不多；但当d_k比较大的时候，如果不使用scale factor，则加法注意力要好一些，因为乘法结果会比较大，容易进入softmax函数的“饱和区”，梯度较小。
对于上述公式的理解，我们可以参考上文的如何使用向量来计算自注意力，将向量替换为矩阵之后得到的公式就是这样的。可以看出和上述公式相同。<br>
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJ1CkX3iaCOf0S9l1sMbiafq6bemLUyETia8RMJkibE6cmwV2SfKdtIDAEfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)<br>
#### 多头注意力
我们发现，使用不同的、学习到的线性投影将查询、键和值分别线性投影到dk、dk和dv维度，而不是使用dmodel维度的键、值和查询来执行单一的注意功能。然后，在这些查询、键和值的投影版本中，我们并行地执行注意功能，生成dv维输出值。如图2所示，这些被连接起来并再次投影，从而产生最终值。多头注意允许模型共同关注来自不同位置的不同表示子空间的信息。只有一个注意力集中的头脑，平均化可以抑制这种情况。在这项工作中，我们使用了h=8个平行的注意层，或者说头部。对于每一个，我们使用dk=dv=dmodel/h=64。由于每个头部的维数减少，总的计算成本与全维度的单头部注意的计算成本相似。
![](https://pic2.zhimg.com/80/v2-478a150bcdc05cf96e713b5b9757a911_720w.webp)
总结：通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：
1. 它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。
2. 它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿X乘以WQ/WK/WV矩阵来产生查询/键/值矩阵。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJYOpsj7mwvqlESJNliaoFpjopMfibzibkhRxsKaIibAXmg0M6zLT9pUVnXA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。但是前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘。
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJmgxPF0eMsPOyibia3dH4eEsYWS2GRfPp2PwNpagfRZrKFuFjrib8esojQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
总的来说就是：
![](https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxUHic7tfPwZwXcMqIqVTXxLJFcS6dNFZEgyaRQNbFKyOJyicjRLduO6Hcr03EmtTOEbJZpBicGdUkJSA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
#### Transformer中注意力的应用
Transformer 以三种不同的方式使用multi-head：
1. 在“编码器-解码器-注意”层中，查询来自上一个解码器层，而内存键和值来自编码器的输出。这使得解码器中的每个位置都可以参与输入序列中的所有位置。这模仿了典型的编码器-解码器-注意机制的顺序对序列模型。
2. 编码器包含self-attention层。在自关注层中，所有键、值和查询都来自同一个位置，在本例中，是编码器中前一层的输出。编码器中的每个位置都可以处理编码器前一层中的所有位置。
3. 类似地，解码器中的 self-attention层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。为了保持解码器的自回归特性，需要防止解码器中的信息向左流动。我们通过屏蔽softmax输入中所有与非法连接相对应的值（设置为–∞）来实现这个内标度点积注意。
### 位置前馈网络
除了注意子层之外，我们的编码器和解码器中的每个层都包含一个完全连接的前馈网络，该网络分别和相同地应用于每个位置。包含中间有着ReLu激活的两个线性层：
![](https://s2.51cto.com/images/blog/202106/15/35f8b2389d626c811a0be0eae827bb83.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp)
### 位置编码
由于我们的模型不包含递归和卷积，为了使模型能够利用序列的顺序，我们必须注入一些关于序列中标记的相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码与嵌入编码具有相同的维度dmodel，因此可以将它们相加。Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。位置编码的公式为：
![](image-2.png)
其中pos是位置，i是维度。也就是说，位置编码的每个维度对应于一个正弦。波长形成了从2π到10000·2π的几何级数。我们选择这个函数是因为我们假设它可以让模型很容易地学会通过相对位置来参与，因为对于任何固定偏移量k，P-Epos+k可以表示为P-Epos的线性函数。
## 为什么要用自注意力
在本节中，我们将self-attention层的各个方面与通常用于映射一个可变长度的符号表示序列（x1，…，xn）到另一个等长序列（Z1，…，Zn）的递归和卷积层进行比较。例如典型的序列转换编码器或解码器中的隐藏层。激发我们使用self-attention，我们考虑三个目的：
* 每层的总计算复杂度。
* 可以并行化的计算量，用所需的最少顺序操作数来衡量。
* 网络中长距离依赖关系之间的路径长度。在许多序列转导任务中，学习长程依赖性是一个关键的挑战。影响学习这种依赖关系的能力的一个关键因素是网络中向前和向后信号必须经过的路径的长度。输入和输出序列中任何位置组合之间的这些路径越短，就越容易学习长期依赖性。因此，我们还比较了在由不同层类型组成的网络中的任意两个输入和输出位置之间的最大路径长度。
## 总结
在这项工作中，**我们提出了第一个完全基于注意的序列转换模型Transformer，它用多头self-attention取代了编码器-解码器架构中最常用的递归层**。在WMT 2014英语-德语和WMT 2014英语-法语翻译任务中，我们达到了一个新的水平。在前一个任务中，我们的最佳模型甚至优于所有先前报告的集成模型。我们对基于注意力的模型的未来感到兴奋，并计划将其应用到其他任务中。我们计划将转换器扩展到涉及文本以外的输入和输出模式的问题，并研究局部的、受限的注意机制，以有效地处理图像、音频和视频等大型输入和输出。减少一代人的先后顺序是我们的另一个研究目标。
